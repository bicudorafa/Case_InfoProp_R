max_depth = 10, # maior profundidade das árvores
early_stopping_rounds = 10, # para após x rounds sem melhora
verbose = 0    # silêncio
)
xgb_cv
xgb_cv %>%
magrittr::use_series(evaluation_log) %>%
summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
ntrees.test  = which.min(test_rmse_mean)) # find the index of min(test_rmse_mean)
imoveis_xgb <- xgboost(data = as.matrix(X_train_treat),
label = y_train,
nrounds = 57,
objective = 'reg:linear',
eta = 0.1,
depth = 10,
verbose = 0
)
# Make predictions
comparacao <- data_frame(pred = predict(imoveis_xgb, as.matrix(X_test_treat)), price = y_test)
comparacao %>%
gather(tipo, valor, price, pred) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density()
comparacao %>%
gather(tipo, valor, price, pred) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
comparacao %>%
mutate(residuals = price - pred) %>%
summarize(rmse = sqrt(mean(residuals**2)))
# Make predictions
comparacao <- data_frame(pred = predict(imoveis_xgb, as.matrix(X_test_treat)), price = y_test)
comparacao %>%
mutate(residuals = price - pred) %>%
summarize(rmse = sqrt(mean(residuals**2)))
prepare
?prepare
treatplan <- designTreatmentsZ(X_train, vars_input, scale = T)
# dfs tratados (opção de colocar argumento scale pra testar com modelos sensitivos como knn, lineares em geral)
X_train_treat <- prepare(treatplan, X_train, varRestriction = novas_vars, scale = T)
summary(X_train_treat)
# dfs tratados (opção de colocar argumento scale pra testar com modelos sensitivos como knn, lineares em geral)
X_train_treat <- prepare(treatplan, X_train, varRestriction = novas_vars)
summary(X_train_treat)
summary(imoveis_final)
View(imoveis_final)
View(X_train_treat)
summary(X_train_treat)
library(dplyr)
library(tidyr)
library(caret)
library(vtreat)
library(xgboost)
set.seed(666)
View(train_fact)
## Pre processamento usando vtreat
treatplan <- designTreatmentsN(train_fact, vars_input, 'price')
treatplan %>%
magrittr::use_series(scoreFrame)
## Pre processamento usando vtreat
treatplan <- designTreatmentsN(train_fact, names(train_fact), 'price')
View(train_fact)
# df Scoreframe para conseguir nomes das novas variáveis
scoreFrame <- treatplan %>%
magrittr::use_series(scoreFrame) %>% # forma dplyr de fazer subsetting "$"
select(varName, origName, code)
novas_vars <- scoreFrame %>%
filter(code %in% c('clean', 'lev')) %>%
magrittr::use_series(varName)
# dfs tratados (opção de colocar argumento scale pra testar com modelos sensitivos como knn, lineares em geral)
train_treat <- prepare(treatplan, train_fact, varRestriction = novas_vars, scale = T)
View(train_treat)
summary(train_treat)
ggplot(train_fact, aes(area)) +
geom_density()
ggplot(train_treat, aes(area_clean)) +
geom_density()
var(train_fact$area)
summary(train_fact$area)
var(train_fact$area, na.rm = T)
var(train_clean$area_clean, na.rm = T)
var(train_treat$area_clean, na.rm = T)
var(X_train_treat$area_clean)
prepare()
prepare
# dfs tratados (opção de colocar argumento scale pra testar com modelos sensitivos como knn, lineares em geral)
train_treat <- prepare(treatplan, train_fact, varRestriction = novas_vars, center = T, scale = T)
View(train_sample)
# Funcao para automatizar normalizacao
scale.features <- function(df, variables){
for (variable in variables){
df[[variable]] <- scale(df[[variable]], center=T, scale=T)
}
return(df)
}
# Transformação do df para modelagem
fact_vars <-  c('rooms_coerc', 'bathrooms_coerc','garage_coerc')
setdiff(names(imoveis_final), fact_vars)
# funcao para transformar variaveis em fact e escalar
toFactor <- function(df, features) {
for (feature in features) {
df[[feature]] <- factor(df[[feature]], exclude = NULL)
# exclude é importantíssimo pra considerar NA como classe
}
return(df)
}
# Funcao para automatizar normalizacao
scale.features <- function(df, variables){
for (variable in variables){
df[[variable]] <- scale(df[[variable]], center=T, scale=T)
}
return(df)
}
# Transformação do df para modelagem
fact_vars <-  c('rooms_coerc', 'bathrooms_coerc','garage_coerc')
imoveis_fact <- toFactor(imoveis_final,fact_vars)
imoveis_scale <- scale.features(imoveis_fact, setdiff(names(imoveis_final), fact_vars))
sample <- createDataPartition(imoveis_scale$price, list = F, p = .7)
train_sample <- imoveis_final[sample, ]
test_sample <- imoveis_final[-sample, ]
View(train_sample)
View(X_test_treat)
# uso do pacte vtreat para one hot encoding
treatPlan <- designTreatmentsN(imoveis_final, names(imoveis_final), 'price')
treatplan %>%
magrittr::use_series(scoreFrame)
treatPlan %>%
magrittr::use_series(scoreFrame)
install.packages('mlr')
library(mlr)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'"),
.funs = funs(as.factor(.))
)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'"),
.funs = funs(as.factor(.))
)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
View(imoveis_fact)
glimpse(imoveis_fact)
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
kable(digits = 2)
# Sumarizando as colunas
summarizeColumns(imoveis_imp)
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2)
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
View(imoveis_norm)
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
View(imoveis_preProc)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
source('~/GitHub/Case_InfoProp_R/ML_mlr.R', encoding = 'UTF-8', echo=TRUE)
View(imoveis_preProc)
devtools::install_github("mlr-org/mlr")
# Criação do Task para Regressão (objeto usado nas operações do pacote)
imoveis_tsk <- makeRegrTask(data = imoveis_preProc, target = 'price')
library(mlr)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
library(mlr)
library(dplyr)
library(tidyr)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
imoveis_tsk <- makeRegrTask(data = imoveis_preProc, target = 'price')
View(imoveis_tsk)
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', imoveis_tsk)
View(holdout)
imoveis_tsk_train <- subsetTask(imoveis_tsk, holdout$train.inds[[1]])
imoveis_tsk_test <- subsetTask(imoveis_tsk, holdout$test.inds[[1]])
listLearners()
listLearners(“regr.”)
listLearners('regr.')
learners <- listLearners()
View(learners)
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
train_lm <- train(regr.lm, imoveis_tsk_train)
View(train_lm)
source('~/GitHub/Case_InfoProp_R/Case_InfoProp.R', encoding = 'UTF-8', echo=TRUE)
View(imoveis_final)
glimpse(imoveis_final)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
#
# Função mais eficiente para featurização (usando dplyr)
#imoveis_fact <- imoveis_final %>%
#  mutate_at(
#    .vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
#    .funs = funs(as.factor(.))
#  )
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_preProc <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_preProc) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Criação da task inicial
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
#
# Função mais eficiente para featurização (usando dplyr)
#imoveis_fact <- imoveis_final %>%
#  mutate_at(
#    .vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
#    .funs = funs(as.factor(.))
#  )
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_final,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_preProc <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_preProc) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Criação da task inicial
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
View(task)
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
# Função mais eficiente para featurização
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
glimpse(imoveis_fact)
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', task)
task_train <- subsetTask(id = 'train_set', task, holdout$train.inds[[1]])
task_test <- subsetTask(id = 'test_set', task, holdout$test.inds[[1]])
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
# Função mais eficiente para featurização
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
glimpse(imoveis_fact)
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', task)
task_train <- subsetTask(task, holdout$train.inds[[1]])
task_test <- subsetTask(task, holdout$test.inds[[1]])
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
train_lm <- train(regr.lm, task_train)
set.seed(666)
# modelo padrão linear
regr.lm <- makeLearner(id = 'lm', 'regr.lm')
xgb_learner <- makeLearner(
"regr.xgboost",
par.vals = list(
objective = "regression",
eval_metric = "RMSE",
nrounds = 200
)
)
# Create an xgboost learner that is classification based and outputs
xgb_learner <- makeLearner("regr.xgboost")
View(xgb_learner)
# treinamento modelo
lm <- train(regr.lm, task_train)
# predição nos dados de teste
pred_lm <- predict(lm, test)
# predição nos dados de teste
pred_lm <- predict(lm, task_test)
View(pred_lm)
performance(pred_lm, measures = c('rmse'))
performance(pred_lm, measures = list('rmse'))
performance(pred_lm)
performance(pred_lm, measures = rmse)
# Create an xgboost learner that is classification based and outputs
xgb_learner <- makeLearner("regr.xgboost")
performance(pred_lm, measures = = list(rmse, mae)))
performance(pred_lm, measures = list(rmse, mae))
# Criação do modelo
xgboost <- makeLearner("regr.xgboost")
# Criação do modelo
xgb_model <- makeLearner("regr.xgboost")
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 500),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 500, upper = 2000),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 2000),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
XGBoost
lm
summary(lm)
