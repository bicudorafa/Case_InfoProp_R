}
# Funcao para automatizar normalizacao
scale.features <- function(df, variables){
for (variable in variables){
df[[variable]] <- scale(df[[variable]], center=T, scale=T)
}
return(df)
}
# Transformação do df para modelagem
fact_vars <-  c('rooms_coerc', 'bathrooms_coerc','garage_coerc')
imoveis_fact <- toFactor(imoveis_final,fact_vars)
imoveis_scale <- scale.features(imoveis_fact, setdiff(names(imoveis_final), fact_vars))
sample <- createDataPartition(imoveis_scale$price, list = F, p = .7)
train_sample <- imoveis_final[sample, ]
test_sample <- imoveis_final[-sample, ]
View(train_sample)
View(X_test_treat)
# uso do pacte vtreat para one hot encoding
treatPlan <- designTreatmentsN(imoveis_final, names(imoveis_final), 'price')
treatplan %>%
magrittr::use_series(scoreFrame)
treatPlan %>%
magrittr::use_series(scoreFrame)
install.packages('mlr')
library(mlr)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'"),
.funs = funs(as.factor(.))
)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'"),
.funs = funs(as.factor(.))
)
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
View(imoveis_fact)
glimpse(imoveis_fact)
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
kable(digits = 2)
# Sumarizando as colunas
summarizeColumns(imoveis_imp)
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2)
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
View(imoveis_norm)
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
View(imoveis_preProc)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
source('~/GitHub/Case_InfoProp_R/ML_mlr.R', encoding = 'UTF-8', echo=TRUE)
View(imoveis_preProc)
devtools::install_github("mlr-org/mlr")
# Criação do Task para Regressão (objeto usado nas operações do pacote)
imoveis_tsk <- makeRegrTask(data = imoveis_preProc, target = 'price')
library(mlr)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
library(mlr)
library(dplyr)
library(tidyr)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
imoveis_tsk <- makeRegrTask(data = imoveis_preProc, target = 'price')
View(imoveis_tsk)
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', imoveis_tsk)
View(holdout)
imoveis_tsk_train <- subsetTask(imoveis_tsk, holdout$train.inds[[1]])
imoveis_tsk_test <- subsetTask(imoveis_tsk, holdout$test.inds[[1]])
listLearners()
listLearners(“regr.”)
listLearners('regr.')
learners <- listLearners()
View(learners)
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
train_lm <- train(regr.lm, imoveis_tsk_train)
View(train_lm)
source('~/GitHub/Case_InfoProp_R/Case_InfoProp.R', encoding = 'UTF-8', echo=TRUE)
View(imoveis_final)
glimpse(imoveis_final)
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
#
# Função mais eficiente para featurização (usando dplyr)
#imoveis_fact <- imoveis_final %>%
#  mutate_at(
#    .vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
#    .funs = funs(as.factor(.))
#  )
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_preProc <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_preProc) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Criação da task inicial
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
#
# Função mais eficiente para featurização (usando dplyr)
#imoveis_fact <- imoveis_final %>%
#  mutate_at(
#    .vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
#    .funs = funs(as.factor(.))
#  )
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_final,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_preProc <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_preProc) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Criação da task inicial
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
View(task)
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
# Função mais eficiente para featurização
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
glimpse(imoveis_fact)
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', task)
task_train <- subsetTask(id = 'train_set', task, holdout$train.inds[[1]])
task_test <- subsetTask(id = 'test_set', task, holdout$test.inds[[1]])
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
## Machine Learning usando pacote MLR
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
# Função mais eficiente para featurização
imoveis_fact <- imoveis_final %>%
mutate_at(
.vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
.funs = funs(as.factor(.))
)
glimpse(imoveis_fact)
## Pacote MLR
# Imputador dos valores missing
imp <- impute(
imoveis_fact,
classes = list(
factor = imputeMode(),
integer = imputeMean(),
numeric = imputeMean()
)
)
imoveis_imp <- imp$data
# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr
# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")
# One hot encoding
imoveis_preProc <- createDummyFeatures(
imoveis_norm, target = "price",
cols = c(
"rooms_coerc",
"bathrooms_coerc",
"garage_coerc"
)
)
# Criação do Task para Regressão (objeto usado nas operações do pacote)
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')
# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', task)
task_train <- subsetTask(task, holdout$train.inds[[1]])
task_test <- subsetTask(task, holdout$test.inds[[1]])
# modelo padrão linear
regr.lm <- makeLearner('regr.lm')
# treinamento modelo
train_lm <- train(regr.lm, task_train)
set.seed(666)
# modelo padrão linear
regr.lm <- makeLearner(id = 'lm', 'regr.lm')
xgb_learner <- makeLearner(
"regr.xgboost",
par.vals = list(
objective = "regression",
eval_metric = "RMSE",
nrounds = 200
)
)
# Create an xgboost learner that is classification based and outputs
xgb_learner <- makeLearner("regr.xgboost")
View(xgb_learner)
# treinamento modelo
lm <- train(regr.lm, task_train)
# predição nos dados de teste
pred_lm <- predict(lm, test)
# predição nos dados de teste
pred_lm <- predict(lm, task_test)
View(pred_lm)
performance(pred_lm, measures = c('rmse'))
performance(pred_lm, measures = list('rmse'))
performance(pred_lm)
performance(pred_lm, measures = rmse)
# Create an xgboost learner that is classification based and outputs
xgb_learner <- makeLearner("regr.xgboost")
performance(pred_lm, measures = = list(rmse, mae)))
performance(pred_lm, measures = list(rmse, mae))
# Criação do modelo
xgboost <- makeLearner("regr.xgboost")
# Criação do modelo
xgb_model <- makeLearner("regr.xgboost")
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 500),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 500, upper = 2000),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
# Grid de alguns parâmetros
xgb_params <- makeParamSet(
# The number of trees in the model (each one built sequentially)
makeIntegerParam("nrounds", lower = 100, upper = 2000),
# number of splits in each tree
makeIntegerParam("max_depth", lower = 1, upper = 10),
# "shrinkage" - prevents overfitting
makeNumericParam("eta", lower = .1, upper = .5),
# L2 regularization - prevents overfitting
makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
XGBoost
lm
summary(lm)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 5) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
# Carregando Pacotes Necessários
library(dplyr)
library(tidyr)
library(mlr)
set.seed(666)
# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 5) # maxit represente tempo máximo de 1 min para iterações
# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)
# tunador dos hiper parâmetros
tuned_params <- tuneParams(
learner = xgb_model,
task = task_train,
resampling = resample_desc,
par.set = xgb_params,
control = control
)
# Modelo tunado
xgb_tuned_model <- setHyperPars(
learner = xgb_model,
par.vals = tuned_params$x
)
# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
# predição nos dados de teste
pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, mae))
performance(pred_xgb, measures = list(rmse, rsq))
performance(pred_lm, measures = list(rmse, rsq))
# Verify performance on cross validation folds of tuned model
resample(xgb_tuned_model,task_train,resample_desc,measures = list(rsq,mse))
# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_model,task_train,resample_desc,measures = list(rmse, rsq))
head(pred_lm)
head(pred_lm[['data']][['response']])
## Análise dos dados fitted e feature importance
comparacao <- data_frame(pred_lm = pred_lm[['data']][['response']], pred_xgb = pred_xgb[['data']][['response']],
price = y_test)
## Análise dos dados fitted e feature importance
comparacao <- data_frame(pred_lm = pred_lm[['data']][['response']], pred_xgb = pred_xgb[['data']][['response']],
price = imoveis_preProc$price)
task_test
View(task_test)
head(getTaskData(task_test)[['price']])
## Análise dos dados fitted e feature importance
comparacao <- data_frame(pred_lm = pred_lm[['data']][['response']], pred_xgb = pred_xgb[['data']][['response']],
price = getTaskData(task_test)[['price']])
View(comparacao)
comparacao %>%
gather(tipo, valor, price, pred_lm, pred_xgb) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
library(ggplot2)
# Plot predictions (on x axis) vs actual bike rental count
comparacao %>%
gather(tipo, valor, price, pred_lm, pred_xgb) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
comparacao %>%
gather(tipo, valor, price, pred_lm, pred_xgb) %>%
filter(!(tipo == pred_xgb)) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
comparacao %>%
gather(tipo, valor, price, pred_lm, pred_xgb) %>%
filter(!(tipo == 'pred_xgb')) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
comparacao %>%
gather(tipo, valor, price, pred_lm, pred_xgb) %>%
filter(!(tipo == 'pred_lm')) %>%
ggplot(aes(valor, fill = tipo)) +
geom_density(alpha = .5)
# Features mais relevantes
mif = generateFilterValuesData(XGBoost, method =
c("information.gain", "chi.squared"))
listFilterMethods
# Features mais relevantes
mif = generateFilterValuesData(XGBoost, method =
c("randomForestSRC.rfsrc"))
# Features mais relevantes
mif = generateFilterValuesData(task_test, method =
c("randomForestSRC.rfsrc"))
