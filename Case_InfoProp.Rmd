---
title: "Case_InfroProp"
author: "Rafael Bicudo Rosa"
date: "28 de agosto de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Business Case - InfoProp

O objetivo deste trabalho é realizar uma análise minuciosa em dados imobiliários da cidade de São Paulo no ano de 2017. Através da limpeza e a vizualização dos relacionamentos entre as variáveis disponíves, a ideia é tentar captar insights importantes e, em seguida, construir um modelo de precificação baseado nas informações encontradas.

## Etapa 1 - Coleta dos Dados

Para iniciar a análise, começarei pela abertura do arquivo através da função base do sistema e, em seguida, com o auxílio do pacote dplyr, tranformá-la-ei em um tbl.df para obter as melhores possibilidades de manipulação de dados.


```{r coleta}
# Carrehando pacote necessario
library(dplyr)

# Obtencao dos dados
imoveis <- as.tbl(read.csv2('Crawler_vivareal_venda_2017_desafio.csv', stringsAsFactors = F, encoding = "UTF-8",
                            na.strings = ""))
glimpse(imoveis)

```


## Etapa 2 - Limpeza e Preparação dos dados

O primeiro passo será a limpeza dos dados. Após realizar a abertura do arquivo, identifiquei as necessidades de correção nas variáveis, das quais se destacam: problema nos encodings das variáveis data, latitude e longitude; e o formato de texto das variáveis numéricas. Os pacotes selecionados para tais tarefas foram o lubridate e stringr. Ao término do processo, percebe-se a diferença no novo dataframe.

```{r normalizando}
### Data celaning
imoveis_clean = imoveis

# Pacotes necessários
library(stringr)
library(lubridate)

# data
imoveis_clean  <-  imoveis

# data
names(imoveis_clean) <- str_replace(names(imoveis_clean), 'X.U.FEFF.date', 'Data')
imoveis_clean$Data <- as.Date(imoveis$X.U.FEFF.date, format = '%d/%m/%Y')
year(imoveis_clean$Data) <- year(imoveis_clean$Data)+2000

# lat e long
imoveis_clean$longitude <- gsub(".", "", imoveis$longitude, fixed = TRUE) %>%
  sub("(\\d{2})", "\\1\\.", .) %>% 
  as.numeric()
imoveis_clean$latitude <- gsub(".", "", imoveis$latitude, fixed = TRUE) %>%
  sub("(\\d{2})", "\\1\\.", .) %>% 
  as.numeric()

# price_by_sqm
imoveis_clean$price_by_sqm <- as.numeric(imoveis_clean$price_by_sqm)

# condominium_fee e iptu
imoveis_clean$condominium_fee <- str_replace_all(imoveis$condominium_fee, 'R\\$', '') %>% 
  str_replace_all('[:punct:]', '') %>% 
  as.numeric()

imoveis_clean$iptu <- str_replace_all(imoveis$iptu, 'R\\$', '') %>% 
  str_replace_all('[:punct:]', '') %>% 
  as.numeric()

# visualizacao do novo df
glimpse(imoveis_clean)
summary(imoveis_clean)
```


## Etapa 3 - Análise Exploratória dos Dados

Com a preparacao dos dados concluída, pode-se prosseguir à análise exploratória. Para tal objetivo, procederemos a uma série de visualizações para ilustrar melhor a relação entre a variável de interesse em relação as demais.


```{r EDA 1}
## Visualizacao interativa
library(leaflet)

leaflet(data = imoveis_clean) %>% addTiles() %>% 
  addMarkers(~longitude, ~latitude, popup = ~as.character(property_d), 
             clusterOptions = markerClusterOptions()
  )

```


A começar com uma visualização interativa dos imóveis no mapa da cidade, através do pacote leaflet, consegue-se obersvar uma série de 20 coordenadas geográficas distantes da maioria das observações, configurando um ponto de interesse para se analisar com mais calma em seguida.


```{r EDA 2}
# visualizacao das variaveis continuas
library(ggplot2)

cont_vars <- c('area', 'iptu','condominium_fee')
plots_cont <- list()
for (var in cont_vars) {
  plots_cont[[var]] <- ggplot(imoveis_clean, aes_string(log(imoveis_clean[[var]]), log(imoveis_clean[['price']]))) + 
    geom_point() +
    geom_smooth(method = lm, se = F) + 
    ggtitle(paste(var, 'x price')) +
    theme_minimal()
  print(plots_cont[[var]])
}
```


Com o auxílio do pacote ggplot2, pode-se vislumbrar a relação gráfica entre as variáveis contínuas do dataframe. É interessante notar a ocorrência de outliers na relação entre 'price', 'area' e 'condominium_fee', gerando outras observações a serem olhadas mais de perto.


```{r EDA 3}
# visualizacao das variaveis factor
fact_vars <- c('bairro', 'rooms', 'bathrooms','garage')
plots_fact <- list()
for (var in fact_vars) {
  plots_fact[[var]] <- ggplot(imoveis_clean, aes_string(x = as.factor(imoveis_clean[[var]]), log(imoveis_clean[['price']]))) +
    geom_boxplot() + 
    ggtitle(paste(var, 'x price')) +
    theme_minimal()
  print(plots_fact[[var]])
}

plots_fact_c <- list()
for (var in fact_vars) {
  plots_fact_c[[var]] <- ggplot(imoveis_clean, aes_string(x = as.factor(imoveis_clean[[var]]))) + 
    geom_bar(stat = 'count') +
    ggtitle(paste('Total Observations in ',var)) +
    theme_minimal()
  print(plots_fact_c[[var]])
}
```


Por fim toma-se nota das relações entre as variáveis categóricas com auxílio do boxplots e gráficos de barra. Praticamente, todas as variáveis possuem algum outlier interessante, ou poucas observações de valores muito elevados, assim se fará uma sensível análise caso a caso para entender melhor suas particularidades. Outro ponto importante é a inexistência de outros bairros a não ser Vila Nova Conceição.


## Etapa 4 - Outliers

Nesta seção, realizaremos um breve estudo dos outliers identificados ao longo da análise exploratória para melhor entender seus aparecimentos.


```{r outliers}
## Investigacao outliers
imoveis_clean %>% filter(log(area) > 10 | log(price) < 10 | log(condominium_fee) > 10) 
imoveis_clean %>% filter(bathrooms == 54)

# outliers de area e price: informações muito discrepantes em relação à realidade. Provavelmente, erros de digitação
imoveis_clean$area[which(imoveis$area == 30700)] <- NA
imoveis_clean <- imoveis_clean %>% filter(!(price == 18000))
imoveis_clean$condominium_fee[which(imoveis$condominium_fee == 460000)] <- NA

# o apartamento com mais de 54 banheiros parece muito fora da media em relacao ao seu preco e area
imoveis_clean$bathrooms[which(imoveis$bathrooms == 54)] <- NA

```


Os valores de 'price', 'area' e 'condominium_fee', assim como vizualizados acima, estavam muito discrepantes da realidade sem motivo aparente, dando uma boa evidência de serem erros de digitação. Por não haver forma melhor de checar sua veracidade, e para não gerar viés indesejado na análise, optei por retirá-los da amostra. Como o único imóvel com um número muito acima da média de banheiros não seguia outros em relação a área e valor, optei pela mesma abordagem também.


## Etapa 5 - Engenharia de Variáveis

Para a construção de um modelo de precificação eficiente, tem-se de extrair o máximo de informação de cada uma das variáveis e, ao mesmo tempo, evitar o overfitting, portanto a construção de novas variáveis é um passo fundamental da análise.

A começar por features já existentes, faz-se a agregação de algumas variáveis categóricas com o intuito de diminuir classes com pouca representatividade na amostra como visto nos gráficos de barra. A ideia é evitar o overfitting.


```{r FE1}
# agregação dos valores para não causar overfitting pela baixa quantidade de valores
roomsBathGarage <- imoveis_clean %>% 
  select(c(rooms, bathrooms, garage, property_d)) %>% 
  mutate(rooms_coerc = ifelse(rooms > 4, 4, rooms),
         bathrooms_coerc = ifelse(bathrooms > 6, 6, bathrooms),
         garage_coerc = ifelse(garage > 5, 5, garage)) %>% 
  select(-c(rooms, bathrooms, garage))
```


Como as variáveis de identificação, em seu estado bruto, não são relevantes à análise, executarei a construção de outras com possibilidade maior de fornecer informação. Após realizar sua criação, faço plots demonstrando suas informações em relação às variáveis antigas.


```{r FE2}
# criacao dos dfs comas supostas novas variaveis de interesse
imoveis_clean_rua <- imoveis_clean %>% 
  group_by(rua) %>%
  summarise(total_a = n(),
            media_p = mean(price_by_sqm, na.rm = T)) %>%
  filter(!(is.na(rua)))

imoveis_clean_condominio <- imoveis_clean %>% 
  group_by(condominio) %>%
  summarise(total_a = n(),
            media_p = mean(price_by_sqm, na.rm = T)) %>%
  filter(!(is.na(condominio))) 

# como ha agencias sem nenhuma entrada, filtro antes de achar as variaves finais
imoveis_clean_agent <- imoveis_clean %>% 
  group_by(agent) %>%
  summarise(total_a = n(),
            media_p = mean(price_by_sqm, na.rm = T)) %>%
  filter(!(is.na(agent))) %>% 
  filter(!(is.na(media_p))) 

# plot total_a
var_t <- list(imoveis_clean_rua %>% arrange(desc(total_a)), 
              imoveis_clean_condominio %>% arrange(desc(total_a)), 
              imoveis_clean_agent %>% arrange(desc(total_a)))

plots_total <- list()

for (i in 1:length(var_t)){
  plots_total[[i]] <- ggplot(var_t[[i]], aes(x = factor(var_t[[i]][[1]], levels = var_t[[i]][[1]]), 
                                             y = total_a)) + 
    geom_bar(stat="identity", width=.5) +
    coord_flip() +
    labs(title=paste('Anúncios x ', names(var_t[[i]])[[1]]), 
         x = names(names(var_t[[i]])[[1]]),
         y = 'Anúncios') 
  
  print(plots_total[[i]])
}

# plot da media_p 
var_m <- list(imoveis_clean_rua %>% arrange(desc(media_p)), 
              imoveis_clean_condominio %>% arrange(desc(media_p)), 
              imoveis_clean_agent %>% arrange(desc(media_p)))

plots_media <- list()

for (i in 1:length(var_m)){
  plots_media[[i]] <- ggplot(var_m[[i]], aes(x = factor(var_m[[i]][[1]], levels = var_m[[i]][[1]]), 
                                             y = media_p)) + 
    geom_bar(stat="identity", width=.5) +
    coord_flip() +
    labs(title=paste('Media x ', names(var_m[[i]])[[1]]), 
         x = names(names(var_m[[i]])[[1]]),
         y = 'Media') 
  
  print(plots_media[[i]])
}

# variaveis criadas
vars_rua <- imoveis_clean_rua %>% select(rua, anuncio_rua = total_a, valores_rua = media_p) 
vars_condo <- imoveis_clean_condominio %>% select(condominio, anuncio_condo = total_a, valores_condo = media_p)  
vars_agente <- imoveis_clean_agent %>% select(agent, anuncio_agente = total_a, valores_agente = media_p) 
```


## Etapa 5 - Dataframe Final para Análise
 
Após terminar a construção de novas variáveis, prossegue-se às etapas de criação do dataset final para análise. Começarei pela junção das novas variáveis, e exclusão das identificadoras, além de 'bairro' que, como visto ao longo de toda análise exploratória, possui variância quase inexistente.


```{r imoveis_final}
# df final para analise e com variaveis novas
imoveis_final <- imoveis_clean %>%
  left_join(roomsBathGarage) %>% 
  left_join(vars_rua) %>% 
  left_join(vars_condo) %>% 
  left_join(vars_agente) %>% 
  select(-c(Data, rua, numero, bairro, price_by_sqm, condominio, rooms, bathrooms, garage,agent, agent_number, 
            latitude, longitude, property_d, url))
  
glimpse(imoveis_final)
```
 

Em seguida, prosseguir-se-á "fatorização" do dataframe, ou seja, transformação das variáveis categóricas para o formato factor


```{r imoveis_factor}
# Função mais eficiente para featurização
imoveis_fact <- imoveis_final %>%
  mutate_at(
    .vars = vars('rooms_coerc', 'bathrooms_coerc','garage_coerc'),
    .funs = funs(as.factor(.))
  )

glimpse(imoveis_fact)
```


## Etapa 6 - Modelos Preditivos


Chega-se à parte mais relevante do trabalho: construção do modelo preditivo para precificação de imóveis. O pacote escolhido para a tarefa foi o "mlr". Atualmente, um dos pacotes mais utilizados por usuários da linguagem, possui muitas opções de customização e facilidade de aplicação.

A primeira etapa consistirá na preparação dos dados através da padronização das variáveis contínuas, o tratamento dos valores missing e o processo conhecido como "One Hot Encoding", ou seja, transformação das variáveis factor em dummies por levels. 


```{r preProcess}
## Pacote MLR

# Pacote
library(mlr)
set.seed(666)

# Imputador dos valores missing
imp <- impute(
  imoveis_fact,
  classes = list(
    factor = imputeMode(),
    integer = imputeMean(),
    numeric = imputeMean()
  )
)
imoveis_imp <- imp$data

# Sumarizando as colunas
summarizeColumns(imoveis_imp) %>%
  knitr::kable(digits = 2) # gerador de tabelas muito prático do knitr

# Normalização
imoveis_norm <- normalizeFeatures(imoveis_imp, target = "price")

# One hot encoding
imoveis_preProc <- createDummyFeatures(
  imoveis_norm, target = "price",
  cols = c(
    "rooms_coerc",
    "bathrooms_coerc",
    "garage_coerc"
  )
)

```


Com os dados preparados para análise, cria-se o objeto básico utilizado neste pacote: task (no caso, uma task específica para regressão). A partir dele, faz-se a separação hold-out (treino x teste)


```{r hold-out}
# Criação do Task para Regressão (objeto usado nas operações do pacote)
task <- makeRegrTask(id = 'Imoveis_SP', data = imoveis_preProc, target = 'price')

# train test split (holdout é o método de resample de separar em train/test)
holdout <- makeResampleInstance('Holdout', task)
task_train <- subsetTask(task, holdout$train.inds[[1]])
task_test <- subsetTask(task, holdout$test.inds[[1]])

```


Com os objetos criados, pode-se iniciar o treinamento dos modelos. Serão usados 2 neste trabalho: um linear simples sem qualquer tipo de tratamento, para se usar como benchmark, e um XGBoost usando validações cruzadas e tunagem de hiper-parâmetros. A escolha do modelo deveu-se pelas relações não lineares entre as variáveis númericas, e seu sistema de uso de gradiente para minimização dos resíduos. 5 validações cruzadas serão utilizadas para evitar o overfitting e a série de hiper-parâmetros escolhidos para tunagem podem ter sua explicação melhor desenvolvida em https://github.com/dmlc/xgboost. 


```{r modelos e score}
## Modelo linear básico

# modelo padrão linear
regr.lm <- makeLearner(id = 'lm', 'regr.lm')

# treinamento modelo
lm <- train(regr.lm, task_train)

## XGBoost

# Criação do modelo simples
xgb_model <- makeLearner("regr.xgboost")

# Grid de alguns parâmetros
xgb_params <- makeParamSet(
  # The number of trees in the model (each one built sequentially)
  makeIntegerParam("nrounds", lower = 100, upper = 2000),
  # number of splits in each tree
  makeIntegerParam("max_depth", lower = 1, upper = 10),
  # "shrinkage" - prevents overfitting
  makeNumericParam("eta", lower = .1, upper = .5),
  # L2 regularization - prevents overfitting
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x)
)

# Controle da tunagem randômica
control <- makeTuneControlRandom(maxit = 1) # maxit represente tempo máximo de 1 min para iterações

# Plano de resampling
resample_desc <- makeResampleDesc("CV", iters = 5)

# tunador dos hiper parâmetros
tuned_params <- tuneParams(
  learner = xgb_model,
  task = task_train,
  resampling = resample_desc,
  measures = rmse,       # R-Squared performance measure, this can be changed to one or many
  par.set = xgb_params,
  control = control
)

# Modelo tunado
xgb_tuned_model <- setHyperPars(
  learner = xgb_model,
  par.vals = tuned_params$x
)

# Verificação da performance do modelo tunado usando as validações cruzadas usadas para tunagem
resample(xgb_tuned_model,task_train,resample_desc,measures = list(rmse, rsq))

# Treinamento
XGBoost <- train(xgb_tuned_model, task_train)
```


## Etapa 7 - Visualização dos Resultados


Agora, com os modelos devidamente treinadss, podemos fazer as comparações dos resultados, quais variáveis possuem maior efeito e como é a sua influência nas previsões. A começar pela performance nos dados de teste, faz-se uma comparação de 2 indicadores de qualidade das previsões juntamente a uma ilustrativa.


```{r performance test}
# predição nos dados de teste
pred_lm <- predict(lm, task_test)
performance(pred_lm, measures = list(rmse, rsq))

pred_xgb <- predict(XGBoost, task_test)
performance(pred_xgb, measures = list(rmse, rsq))

## Análise dos dados fitted e feature importance
comparacao <- data_frame(pred_lm = pred_lm[['data']][['response']], pred_xgb = pred_xgb[['data']][['response']],
                         price = getTaskData(task_test)[['price']])

# Plot predictions (on x axis) vs actual bike rental count
comparacao %>%
  tidyr::gather(tipo, valor, price, pred_lm, pred_xgb) %>%
  filter(!(tipo == 'pred_xgb')) %>% 
  ggplot(aes(valor, fill = tipo)) +
  ggtitle('LM') +
  geom_density(alpha = .5)

comparacao %>%
  tidyr::gather(tipo, valor, price, pred_lm, pred_xgb) %>%
  filter(!(tipo == 'pred_lm')) %>% 
  ggplot(aes(valor, fill = tipo)) +
  ggtitle('LM') +
  geom_density(alpha = .5)

```

Como demonstrado acima, e já esperado, o modelo xgbost apresentou índices superiores nos indicadores, além de, visualmente, ter uma aderência maior aos dados de teste. Portnato, para encerramento da análise, usarei o pacote 'iml', recomendado pelos criadores do 'mlr', para aumentar a compreensão desse modelo mais complexo. Primeiro, descobrindo qual a importância de cada uma das features e, em seguida, como os valores da mais importante influenciam os valores previstos pelo modelo.


```{r iml}
# Pacote
library(iml)
# usando Predictor$new() para criar um plot das features mais importantes
X = getTaskData(task_train)[getTaskFeatureNames(task_train)]
predictor = Predictor$new(XGBoost, data = X, y = getTaskData(task_train)['price'])
imp = FeatureImp$new(predictor, loss = "rmse")
plot(imp)

# Plot de como a variável mais importante afeta as previsões na média
pdp_area = Partial$new(predictor, feature = "area")
plot(pdp_area)
```

## FIM